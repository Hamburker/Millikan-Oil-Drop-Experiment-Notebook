{
  "cells": [
    {
      "metadata": {
        "_uuid": "287e57b6b582cda354131427d6e959ef320bda42"
      },
      "cell_type": "markdown",
      "source": "# The Millikan Oil Drop Experiment\nIn this experiment performed in my Physics lab class my partner and I use tiny droplets of oil in an electric field to measure the value of the elementary electric charge.\n\n## Introduction\nThe Millikan oil drop experiment is a famous physics experiment first performed in 1909 by Robert A. Millikan and Harvey Fletcher to measure the elementary electric charge $e$. The experiment consists of measuring the rise and fall velocities of tiny droplets of oil in an electric field. These droplets are so small that in the absence of an external force they reach their terminal fall velocity almost instantly. By ionizing these droplets with an alpha radiation source we can give them a small positive charge, so that when we switch on an electric field we can cause them to rise with a velocity proportional to their charge. \n\nStoke's Law gives the frictional force exerted on spherical objects with very small Reynolds numbers in a viscous fluid â€“ like our oil droplet moving through air. At terminal velocity the force of gravity is balanced against the frictional force of the air, and since we know both the gravitational constant $g$ and the coefficient of friction for air $k$ (as a function of air pressure and temperature), by measuring the terminal velocity $v_f$ of the droplet we can use Stoke's Law to solve for its mass. With its mass known, we can measure the total charge of the droplet by observing its motion in an applied electric field of a known strength. We can even change the charge of a single droplet by reionizing it and observing its change in velocity. Once we have collected enough data, **we expect to find that the measured charge values of the droplets are integer multiples of some elementary constant.**\n\nIn his original experiment Millikan obtained his velocity measurements by watching these oil droplets move up and down and measuring the time they took to move a known distance. This is no easy task. One must peer through a backlit microscope lens and keep their eye trained on a single microscopic droplet for as long as they can bear, or until it loses its charge and falls out of view. After attempting this classic method for about 20 minutes I decided it wasn't worth losing vision in my right eye over and opted to make use of some of the 21st century technology Millikan lacked access to. In our version of the experiment we used an iPhone 8 camera to record footage of the droplets moving up and down. Additionally, rather than timing these droplets by hand to get their velocity data, I had the idea to automate this as well by motion tracking the droplet footage using one of my all time favorite pieces of open source software, Blender. The tracking went beautifully, and this is primary dataset we will be using in this analysis. "
    },
    {
      "metadata": {
        "_uuid": "518b672830778235475196fa5aa7eff5f6c2e99e"
      },
      "cell_type": "markdown",
      "source": "# Data Analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c8257385803e00cb058ee2111692a4bf4356c8c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nfrom scipy.optimize import curve_fit as fit\nfrom skimage import io\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94aa91fa3052a819650c82fdf1b099715a3ea871"
      },
      "cell_type": "markdown",
      "source": "## Getting data into notebook\nWe took good footage of three separate droplets. The first lasted for about 15 minutes, the second for over 20 minutes, and the third for over 30 minutes. I'll begin by importing the tracking data for these three droplets into my notebook and taking a look at the head of the first dataset to show how they're organized"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d460e5c238e0fe7035aa7e211720cc7ccb339928",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "drop1=pd.read_csv('../input/millikan/trial1_track.csv')\ndrop2=pd.read_csv('../input/millikan/trial2_track.csv')\ndrop3=pd.read_csv('../input/millikan/trial3_track.csv')\n\ndrop1.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "579fe8dde310996c8f9d033602c98eb1ff4f585f"
      },
      "cell_type": "markdown",
      "source": "For each frame there is an x and y coordinate corresponding to the pixel value of the tracking marker for the given frame. One thing to note is that when importing the video file into Blender the video was rotated and the **x and y values were flipped**. So the x coordinate corresponds to the vertical direction, not the horizontal. \n\n# Getting Rise & Fall Velocities\n## Finding peaks and valleys\nThe plan is to slice my dataset containing the entire journey of the particle into individual rises and falls. Then I can fit a line to each one of these segments to get the slope, and therefore the velocity. This should give me more accurate velocity measurements than simply looking at the individual turning points since it uses the entire dataset, rather than a small number of individual turning points. \n\nUsing some blackbelt Google-fu I managed to find a peak finding algorithm in the Scipy package, which I have already imported. I'll be using this to find the peaks and valleys of my vertical position. These are the points where we turned the field off or on. \n\nI'll begin with the first dataset, since it is the smallest, so I can figure out the method. Then I'll move on to the more robust second and third datasets."
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "_uuid": "82a085a86135f6dd22b83df2393cf0a757493dea",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "peak1 = signal.find_peaks_cwt(drop1['x'], np.arange(1,300)) #Finding peaks\nval1 = signal.find_peaks_cwt(-drop1['x'], np.arange(1, 500)) #Finding valleys\n\npeak1 = np.delete(peak1, [0, peak1.size-1]) # Deleting false positives\nval1 = np.delete(val1, 0)\n\nf, ax = plt.subplots(figsize=(10, 5)) #Plotting journey along with peak and valley points\nax.scatter(drop1['frame'], drop1['x'])\nax.scatter(peak1, [drop1.iloc[i]['x'] for i in peak1])\nax.scatter(val1, [drop1.iloc[i]['x'] for i in val1])\nax.set(title='Rise and fall data for Droplet 1', xlabel='Frame number', ylabel='Pixel coordinate')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5f00bea96cb6cee12ac9e7330dbe405ceeb4689a"
      },
      "cell_type": "markdown",
      "source": "It's not perfect, but the peak finding algorithm seems to have done its job pretty well. I had to get rid of some of the poorer datapoints, those being the first and last of the 'peaks' and the first 'valley', where the algorithm seemed to falter.\n\nBelow I have created two lists containing the indices for all of the frames where the particle is rising and all of the frames where the particle is falling. For the plot, pick a number for `num` and it will plot that particular rise and fall. *Reminder: Due to how the video was rotated, falling x values correspond to the droplet actually rising*. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9077c0d81e18cd6074d4cc65cd479be259592d25",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "rises1 = [np.arange(peak1[i], val1[i]) for i in range(10)]\nfalls1 = [np.arange(val1[i], peak1[i+1]) for i in range(9)]\n\nf, ax = plt.subplots(figsize=(15, 5))\nnum=1 # Pick a number in the range 1-10\n\nax.plot([drop1.iloc[i]['frame'] for i in rises1[num]], \n        [drop1.iloc[i]['x'] for i in rises1[num]])\n\nax.plot([drop1.iloc[i]['frame'] for i in falls1[num]], \n        [drop1.iloc[i]['x'] for i in falls1[num]])\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e555425ef09b387be1f766f19b7346b70c192b04"
      },
      "cell_type": "markdown",
      "source": "## Fitting line segments to get velocities\nNow that I have a way of segmenting my data, I can fit a simple line to each one of these segments and record the slope for each fit. First I'll need to create a linear function to be fitted. This will take as its input a droplet dataframe and list of frames corresponding to a given rise or fall, and it will output the slope of that segment."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "01ed7d21d25487b3c910b5743735592a721c402c"
      },
      "cell_type": "code",
      "source": "def linefit(df, rang): \n    popt, pcov = fit(lambda x, a, b: a*x+b, [df.iloc[i]['frame'] for i in rang], [df.iloc[i]['x'] for i in rang])\n    return popt[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a65b9366aea2c0468f05b20fce028cadb104661",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "rises1[1].shape\nvup1 = [linefit(drop1, rises1[i]) for i in range(len(rises1))]\nvdown1 = [linefit(drop1, falls1[i]) for i in range(len(falls1))]\n\nf, ax = plt.subplots(figsize = (15, 5))\nax.bar(np.arange(len(vup1)), vup1)\nax.bar(np.arange(len(vdown1)), vdown1)\nax.set(title = 'Velocity of Rises and Falls', xticks=np.arange(10), ylabel='Velocity (Pixels/Frame)')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c52f1e42d16337df1e54277e0e86f9e934ac5517"
      },
      "cell_type": "markdown",
      "source": "Now we're talking! We clearly see the predicted behavior. The fall velocity (the positive values in the graph) remain basically steady throughout the experiment, while the rise velocity rises stepwise, demonstrated expected behaviour for discrete units of charge increase. \n\n## Repeating method with other two datasets\n### Second droplet\nBefore I continue on with this dataset, let's take a look at the other two, larger datasets to see if we see this same trend."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "12436c9528f3e964d65ecb962c4c3410b60da319"
      },
      "cell_type": "code",
      "source": "peak2 = signal.find_peaks_cwt(drop2['x'], np.arange(1, 700))\nval2 = signal.find_peaks_cwt(-drop2['x'], np.arange(1, 700))\n\n# Getting rid of outliers\npeak2 = np.delete(peak2, [9, 19, peak2.size-1])\nval2 = np.delete(val2, [9, 19])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f7341ec0c09a4b7dec41bd5076b373add403b04",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(15, 5))\nax.scatter(drop2['frame'], drop2['x'])\nax.scatter(peak2, [drop2.iloc[i]['x'] for i in peak2])\nax.scatter(val2, [drop2.iloc[i]['x'] for i in val2])\nax.set(title='Positional data in vertical direction of 20 minute trial', \n       ylabel='Position in vertical direction in pixels', \n       xlabel='Frame')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f3b7c7035b78161eae8efb2e485e6ea02089f322"
      },
      "cell_type": "markdown",
      "source": "Pretty nice looking peaks once the outliers have been removed! I'll use the same code from above to allow myself to look at the individiual rises and falls more closeley. Then I will use the `linefit` function to get some velocities out of this data and see how they look."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9512e486959f82d874f7e51fa7beeecd804b538",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "rises2 = [np.arange(peak2[i], val2[i]) for i in range(len(peak2))]\nfalls2 = [np.arange(val2[i], peak2[i+1]) for i in range(len(peak2)-1)]\n\nf, ax = plt.subplots(figsize=(15, 5))\nnum = 20 #Max num is 32\n\nax.plot([drop2.iloc[i]['frame'] for i in rises2[num]], [drop2.iloc[i]['x'] for i in rises2[num]])\nax.plot([drop2.iloc[i]['frame'] for i in falls2[num]], [drop2.iloc[i]['x'] for i in falls2[num]])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "c79e3e95ee1aeb198a2d754f210e6076a35a8287"
      },
      "cell_type": "code",
      "source": "def linefit(df, rang): \n    popt, pcov = fit(lambda x, a, b: a*x+b, [df.iloc[i]['frame'] for i in rang], [df.iloc[i]['x'] for i in rang])\n    return popt[0], np.sqrt(pcov[0,0]*pcov[0,0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "1e9ecb0072026d8812ccd73ae5dc93f4508535fc"
      },
      "cell_type": "code",
      "source": "vup2 = [linefit(drop2, rises2[i])[0] for i in range(len(rises2))]\nvuperr2 = [linefit(drop2, rises2[i])[1] for i in range(len(rises2))]\nvdown2 = [linefit(drop2, falls2[i])[0] for i in range(len(falls2))]\nvdownerr2 = [linefit(drop2, falls2[i])[1] for i in range(len(falls2))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e962ab277150b3a930b885600b4ef5c864c955cb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize = (15, 10))\nax.bar(np.arange(len(vup2)), vup2, yerr = vuperr2)\nax.bar(np.arange(len(vdown2)), vdown2, yerr = vdownerr2)\nax.set(ylabel='Velocity (Pixels/Frame)', xticks=np.arange(len(vup2)))\nax.legend(['Rises', 'Falls'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7dddfbd65fdc7e627b0508b605e307e6e0f29247"
      },
      "cell_type": "markdown",
      "source": "While the fall velocity appears to vary a bit more than it did in the last dataset, it still remains around some constant value. We again see the discrete behavior of the rising velocities. \n\n### Third Dataset\nNow to repeat the process once more for the third dataset."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "6440fa368b30471ccf7afbdd268b70ab71c02171"
      },
      "cell_type": "code",
      "source": "peak3a = signal.find_peaks_cwt(drop3['x'], np.arange(1, 700))\nval3a = signal.find_peaks_cwt(-drop3['x'], np.arange(1, 700))\n#The 'a' at the end is to differentiate these peaks and valleys from the\n#corrected peaks and valleys which I will create later, which have the \n#outliers removed. This way if I run that code again it wont remove values\n#that I want to keep.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e71316c7850eb2ba02f813b3b785f3f8a8a5d3c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(20, 10))\nax.scatter(drop3['frame'], drop3['x'])\nax.scatter(peak3a, [drop3.iloc[i]['x'] for i in peak3a])\nax.scatter(val3a, [drop3.iloc[i]['x'] for i in val3a])\n\nfor i in range(len(peak3a)): \n    ax.annotate('{0}'.format(i), xy=(peak3a[i], drop3.iloc[peak3a[i]]['x']))\nfor i in range(len(val3a)): \n    ax.annotate('{0}'.format(i), xy=(val3a[i], drop3.iloc[val3a[i]]['x']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8377e4069c397b6c66c99789d62b746de75cc3b5"
      },
      "cell_type": "markdown",
      "source": "This dataset is both larger and a bit messier than the previous two, so I've numbered the points on the graph to make things easier on myself. The obvious points I'll have to get rid of are index numbers 6 and 28 for both the rise and fall groups. There is also some strange behaviour around 14 and 34. I'll begin by removing the obvious ones, and then after calculating the velocities I'll remove the velocities which are innacurate due to erratic tracker behaviour."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "751838c3676ee7401e36be2956a0313b6d63f33f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "peak3 = np.delete(peak3a, [6, 28, len(peak3a)-1])\nval3 = np.delete(val3a, [6, 28])\n\nf, ax = plt.subplots(figsize = (20, 10))\nax.scatter(drop3['frame'], drop3['x'])\nax.scatter(peak3, [drop3.iloc[i]['x'] for i in peak3])\nax.scatter(val3, [drop3.iloc[i]['x'] for i in val3])\n\nfor i in range(len(peak3)): \n    ax.annotate('{0}'.format(i), xy=(peak3[i], drop3.iloc[peak3[i]]['x']))\nfor i in range(len(val3)): \n    ax.annotate('{0}'.format(i), xy=(val3[i], drop3.iloc[val3[i]]['x']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82311231b0dad87f002f77532c97a4529001425f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "rises3 = [np.arange(peak3[i], val3[i]) for i in range(len(peak3))]\nfalls3 = [np.arange(val3[i], peak3[i+1]) for i in range(len(peak3)-1)]\n\nf, ax = plt.subplots(figsize=(15, 5))\nnum= 10 # Change to view different rises/falls\nax.plot([drop3.iloc[i]['frame'] for i in rises3[num]], [drop3.iloc[i]['x'] for i in rises3[num]])\nax.plot([drop3.iloc[i]['frame'] for i in falls3[num]], [drop3.iloc[i]['x'] for i in falls3[num]])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": true,
        "_uuid": "3d39804145353ba4c0dbdcaf8cc44e67cfda1c78"
      },
      "cell_type": "code",
      "source": "vup3 = [linefit(drop3, rises3[i])[0] for i in range(len(rises3))]\nvdown3 = [linefit(drop3, falls3[i])[0] for i in range(len(falls3))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3c9f943cefa8e0d5fe2b45ab6700612e2a22f64",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize = (15, 10))\nax.bar(np.arange(len(vup3)), vup3)\nax.bar(np.arange(len(vdown3)), vdown3)\nax.set(xticks=np.arange(len(vup3)))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e6cd0873c5f1dc5267a2984df5d792c0bbb8ce0f"
      },
      "cell_type": "markdown",
      "source": "# Getting Uncertainty Data\n## Finding average width of the droplet\nI would like to use Scikit to automate the random sampling and measurement of the droplet dimensions. I'll start with a single image from my second trial dataset just to get my feet wet with this new toolkit. \n\nI will be randomly selecting images from the image sets of the trials and using the tracking data to locate the vicinity of the droplet in the image. Then I will hopefully be able to measure the size of the droplet from this image data.\n\n### Second dataset\nOne thing to note is that my image set contains all of the frames from the footage we took, but the tracking data does not begin from the first frame. For the first set of data, the tracking does not begin until the 678th frame, so when retrieving the coordinates I must make sure to account for this shift. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db1ce936b7324c311ca977edaa60ad937ab2a91c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "frame = 20\nim = io.imread('../input/trial-2-image-set/trial 2 image set/Trial 2 Image Set/{0}{1}.jpg'.format('0'*(4-len(str(frame+678))), frame+678))\npat = im[-int(drop2.iloc[frame]['y'])-10:-int(drop2.iloc[frame]['y'])+10, int(drop2.iloc[frame]['x'])-10:int(drop2.iloc[frame]['x'])+10]\n\npat[pat<0.6*pat.max()]=0 # Mask value\nplt.imshow(pat)\nplt.colorbar()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5a36f6abaa3a925b2a94c3d2b6adf317b5762278"
      },
      "cell_type": "markdown",
      "source": "After playing around with the index values for a while, I've at last managed to pull up the coordinates of the droplet for a given frame and display the area of the Blender tracking box. This 20x20 area is the marker 'pattern', or box which gets displayed in the UI to help find the tracker (the actual pattern box is 21x21 close enough). Since the droplet moves so slowly it never actual leaves this box for any given frame, so its a great area to limit my size calculations to.\n\nBy adjusting the above mask we can constrain the displayed values to ones above some lower limit. Changing the `frame` value displays the droplet for different frames. Experimenting with these two parameters, I found that a mask value of 60% of the maximum pixel value did the best job of isolating the droplet in the image. By measuring the standard deviation of these pixels away from the recorded tracking point I should be able to get a very good value for the uncertainty of my positional data."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "f1f7aa0fb9493b01f6f03bf30f42375aa5df2efe"
      },
      "cell_type": "code",
      "source": "def track2_std(frame):\n    im = io.imread('../input/trial-2-image-set/trial 2 image set/Trial 2 Image Set/{0}{1}.jpg'.format('0'*(4-len(str(frame+678)))*(len(str(frame+678))<5), frame+678))\n    pat = im[-int(drop2.iloc[frame]['y'])-10:-int(drop2.iloc[frame]['y'])+10, int(drop2.iloc[frame]['x'])-10:int(drop2.iloc[frame]['x'])+10]\n    y, x = np.where(pat > 0.6*pat.max())\n    stdx = np.sqrt(np.mean(np.array([(i-10)*(i-10) for i in x])))\n    stdy = np.sqrt(np.mean(np.array([(i-10)*(i-10) for i in y])))\n    return stdx, stdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2d78ad5472b777a050e8dec70e64fe05382b65b9"
      },
      "cell_type": "markdown",
      "source": "The above function takes as its input a frame number and outputs the standard deviation in the x and y directions of the pixels in the tracking pattern with values greater than 60 away from the recorded tracking coordinate for the frame (for the second data set). \n\nSadly I cannot locate the Blender file I used to track the first trial set, and so I do not know which frame I started the tracking for that video. I do have the tracking file for the second and third sets though, so I should be able to get good data from those two. \n\nNow I need to sample my frame image data and use the standard deviation function defined above to get the average standard deviation of the droplet location from the recorded tracker value. I'll take a sample from each second of footage, or one every thirty frames."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "7ea2de13647a15ef5990a7e86ec297183c867dcd"
      },
      "cell_type": "code",
      "source": "sample = np.arange(0, len(drop2), 30)\n\nstdx = [track2_std(int(s))[0] for s in sample]\nstdy = [track2_std(int(s))[1] for s in sample]\n\nprint('Standard deviation of droplet pixels from tracking point: \\n Vertical direction: {0} \\n Horizontal direction: {1}'.format(np.average(stdx), np.average(stdy)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "b9b27357308199a155c5e72eca825c98d0c28f46"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots()\nax.hist(stdy, alpha = 0.5, bins=10)\nax.hist(stdx, alpha = 0.5, bins=10)\nax.set(title='Standard deviation of droplet pixels from tracking point', \n       xlabel='Standard deviation of droplet pixels from tracker location (pixels)', ylabel='Count number')\nax.legend(['Horizontal direction', 'Vertical direction'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2f17a01b7b6a551acb825a97f447507ef3a94e6"
      },
      "cell_type": "markdown",
      "source": "The standard deviation is greater in the vertical direction, which is expected given that this is the primary direction of motion for the droplet. From the histogram we can see how the horizonatl values are much more clustered around a standard deviation of two, while the vertical values are more smeared out.\n\nNow I would like to see how these standard deviations vary with time. I'll use a scatter plot to visualize all of my sampled datapoints, and then use a bar chart to show the average standard deviation in the vertical direction over 10 second intervals (each of which should have 30 data points because I collected one sample per second)."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "8c8eee6283c9fe1316070a5fe947036a9370da3b"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(20, 10))\n\nax.scatter(sample, stdx, alpha =0.5)\n\nnum=int(drop2.frame.max()/300) # Number of bars\nbins = np.linspace(0, sample.max(), num)\nvals = [np.average([stdx[i] for i in np.where((sample >bins[i]) & (sample < bins[i+1]))[0]]) for i in range(len(bins)-1)]\nax.bar(left=bins[:-1], height=vals, width=(2/3)*sample.max()/num, zorder=0, align='edge')\n\nax.set(xlim=[0, drop2.frame.max()], ylabel='Standard deviation of droplet from tracker position', \n       xlabel='Frame number', title='Standard deviation of droplet pixels from tracking point versus frame number')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "875a2a3a262be744bbccd0b2e5cdcbc4a3b00e39"
      },
      "cell_type": "code",
      "source": "sigma2_rise = np.array([[stdx[int(np.where((i >= sample) & (i < sample+30))[0])] for i in rises2[j]] for j in range(len(rises2))])\nsigma2_fall = np.array([[stdx[int(np.where((i >= sample) & (i < sample+30))[0])] for i in falls2[j]] for j in range(len(falls2))])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "f5266a1bf21a6321bda77e1112bfddf1859fd53c"
      },
      "cell_type": "code",
      "source": "vup2 = [linefit(drop2, rises2[i])[0] for i in range(len(rises2))]\nvuperr2 = [linefit(drop2, rises2[i])[1] for i in range(len(rises2))]\nvdown2 = [linefit(drop2, falls2[i])[0] for i in range(len(falls2))]\nvdownerr2 = [linefit(drop2, falls2[i])[1] for i in range(len(falls2))]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "257314b58b37de57a4dde3ba5451eaaba3f24aa0"
      },
      "cell_type": "markdown",
      "source": "We can see from the above graph of the standard deviation of the sampled frames how the uncertainty of the tracker position varies throughout the footage. There are higher levels of variation between frames ~2500 and ~7500, and also in the final third of the footage, beginning near frame 20000. The most precise portion of the footage is the middle section, when the droplet location is very near the position of the tracker.\n\nWith this information, I should be able to get very accurate fits to my data and an idea of how precise my results are. "
    },
    {
      "metadata": {
        "_uuid": "376326d0c87aa943bb1d675db5e84073425b23dd"
      },
      "cell_type": "markdown",
      "source": "### Third dataset\nFrom the Blender file, this third tracking dataset begins at the 21st frame, so I need to shift the frame data by this number. The images in this set are darker than those of the first, and the droplet is smaller. To account for this, I used the slightly less restrictive mask value of 50% of whatever the maximum pixel value is for a given search pattern, rather than 60%."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09caf6455afb8ba8494bb8b8ad749cd957fac676",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "frame = 10000\nim = io.imread('../input/trial-3-images/trial 3 image set/Trial 3 Image Set/{0}{1}.jpg'.format('0'*(4-len(str(frame+21))), frame+21))\npat = im[-int(drop3.iloc[frame]['y'])-10:-int(drop3.iloc[frame]['y'])+10, int(drop3.iloc[frame]['x'])-10:int(drop3.iloc[frame]['x'])+10]\n\npat[pat<0.5*pat.max()]=0 # Mask value\nplt.imshow(pat)\nplt.colorbar()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "d95b364b049c7359a8e42fc902b188c1ba900a40"
      },
      "cell_type": "code",
      "source": "def track3_std(frame):\n    im = io.imread('../input/trial-3-images/trial 3 image set/Trial 3 Image Set/{0}{1}.jpg'.format('0'*(4-len(str(frame+21)))*(len(str(frame+21))<5), frame+21))\n    pat = im[-int(drop3.iloc[frame]['y'])-10:-int(drop3.iloc[frame]['y'])+10, int(drop3.iloc[frame]['x'])-10:int(drop3.iloc[frame]['x'])+10]\n    y, x = np.where(pat > 0.6*pat.max())\n    stdx = np.sqrt(np.mean(np.array([(i-10)*(i-10) for i in x])))\n    stdy = np.sqrt(np.mean(np.array([(i-10)*(i-10) for i in y])))\n    return stdx, stdy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "05e283b3afd8eacb5c561b9813b3493ad4b7b020"
      },
      "cell_type": "code",
      "source": "sample = np.arange(0, len(drop3), 30)\n\nstdx = [track3_std(int(s))[0] for s in sample]\nstdy = [track3_std(int(s))[1] for s in sample]\n\nprint('Standard deviation of droplet pixels from tracking point: \\n Vertical direction: {0} \\n Horizontal direction: {1}'.format(np.average(stdx), np.average(stdy)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "9334db914e01132bf8e145c9ca27c90ef41c2ca1"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots()\nax.hist(stdy, alpha = 0.5)\nax.hist(stdx, alpha = 0.5)\nax.set(title='Standard deviation of droplet pixels from tracking point', \n       xlabel='Standard deviation of droplet pixels from tracker location (pixels)', ylabel='Count number')\nax.legend(['Horizontal direction', 'Vertical direction'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bcbc3c69722705061b813bd9a512a4784a1f426a"
      },
      "cell_type": "markdown",
      "source": "Once again, we see that the vertical has a higher average standard deviation from the tracker position than the horizontal direction. The droplet in this trial is generally more concentrated around the tracked point for this dataset than for the last. This is a result of the smaller size of the droplet used for this trial. "
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "8d6d250e4151966b8225125e37115caea7e385da"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(20, 10))\nax.scatter(sample, stdx, alpha =0.5)\n\nnum=int(drop3.frame.max()/300) \nbins = np.linspace(0, sample.max(), num)\nvals = [np.average([stdx[i] for i in np.where((sample >bins[i]) & (sample < bins[i+1]))[0]]) for i in range(len(bins)-1)]\nax.bar(left=bins[:-1], height=vals, width=(2/3)*sample.max()/num, zorder=0, align='edge')\nax.set(xlim=[0, drop3.frame.max()], ylabel='Standard deviation of droplet from tracker position', xlabel='Frame number')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "43817fffaef1433ba9d90cb4152e1a41c19bb335"
      },
      "cell_type": "markdown",
      "source": "From looking at this sample of pixel distrubion standard deviations it seems as though this trial had a lower and more consistent tracking precision than the last one. There are a few differences of note between this graph and the last one.\n- There are sampled values which have a stardard distribution of zero in the vertical direction, which was never the case for the second trial.\n- The results as a whole seem much more uniform, meaning that many samples seem to have the exact same value. For smaller droplets with fewer pixels on average this is somewhat expected. \n- There is a very clear peak in the middle section where the standard deviation is larger than normal, before returning to about where it was before. \n\nLet's see what happens at these two extremes: unusually high and unusually low standard deviations."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "e97aee99489207a5d5492ffc45d6ef76487781b0"
      },
      "cell_type": "code",
      "source": "print('Frame with highest vertical standard deviation: #{0}'.format(int(sample[np.where(stdx == max(stdx))])))\n\nframe = int(sample[np.where(stdx == max(stdx))])\nim = io.imread('../input/trial-3-images/trial 3 image set/Trial 3 Image Set/{0}{1}.jpg'.format('0'*(4-len(str(frame+21))), frame+21))\npat = im[-int(drop3.iloc[frame]['y'])-10:-int(drop3.iloc[frame]['y'])+10, int(drop3.iloc[frame]['x'])-10:int(drop3.iloc[frame]['x'])+10]\npat[pat<0.5*pat.max()]=0 # Mask value\n\nf, ax = plt.subplots()\nplt.imshow(pat)\nplt.colorbar()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b0cfd2d6abe6ab28489484af78c26a49a4041ebc"
      },
      "cell_type": "markdown",
      "source": "So it appears that the tracker simply got away from the droplet a bit. Not a big problem. Let's see what happened at all the points with no standard deviation."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "255ca37ff72a66901be7a42fce10f9f939868a0d"
      },
      "cell_type": "code",
      "source": "print('Frames with lowest vertical standard deviation:\\n{0}.'.format(', '.join([str(i) for i in sample[np.where(stdx == min(stdx))]])))\n\nzeros = sample[np.where(stdx == min(stdx))]\n\nframe = zeros[11] # Pick any number between 0 and 11\n\nprint('\\nDisplaying frame #%i:' % frame)\n\nim = io.imread('../input/trial-3-images/trial 3 image set/Trial 3 Image Set/{0}{1}.jpg'.format('0'*(4-len(str(frame+21))), frame+21))\npat = im[-int(drop3.iloc[frame]['y'])-10:-int(drop3.iloc[frame]['y'])+10, int(drop3.iloc[frame]['x'])-10:int(drop3.iloc[frame]['x'])+10]\npat[pat<0.5*pat.max()]=0 # Mask value\n\nplt.imshow(pat)\nplt.colorbar()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8970f5e9af83930647f13710902153072d80f880"
      },
      "cell_type": "markdown",
      "source": "These frames are somewhat confusing to me, since most of them seem like they should have a standard x deviation of greater than zero. In any case though, it seems that their contribution to the uncertainty will be minimal, and I don't want to waste too much time focusing on this."
    },
    {
      "metadata": {
        "_uuid": "c64f457a6a7f6587831ee8d759289a36ab3cb76a"
      },
      "cell_type": "markdown",
      "source": "## Background velocity fluctuations\nI now want to find out what the impact of brownian motion and other small, local forces are on the motion of the droplets. I'll do this by comparing the fluctuations in the vertical direction (x direction of the footage) to the fluctuations in the horizontal direction (y direction of the footage). In theory there should be no net forces in the horizontal direction, so the droplets random motion in this direction should give us information on what kinds of variations in velocity are considered normal in our test conditions. \n\n### Second Droplet\nFirst, I'll get a simple idea of the time variation in the velocities and accelerations of the droplet in the horizontal direction by simply plotting their variations on a frame by frame basis over the course of the videos. "
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "a3110a5c2099c74866c858b71fba56781ce69292"
      },
      "cell_type": "code",
      "source": "yvel = drop2.y.shift(-1)-drop2.y # Y velocity in one frame (dx)\n# Printing average magnitude of yvel (excluding final 100 frames)\nprint('Average y velocity magnitude (dy): {0}'.format(np.average(np.sqrt(yvel[:-100]*yvel[:-100])))) \nf, ax = plt.subplots(figsize=(15, 10))\nyacc = yvel.shift(-1)-yvel # Y acceleration over one frame (dx^2)\n#Printing average magnitude of yacc (excluding final 100 frames)\nprint('Average y acceleration magnitude (dy^2): {0}'.format(np.average(np.sqrt(yacc[:-100]*yacc[:-100]))))\nax.plot(yvel, alpha = 0.5)\nax.plot(yacc, alpha=0.5)\nax.legend(['Velocity', 'Acceleration'])\nax.set(ylim = [-5, 5])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4e041ac121d11377470891d6cee3cff6547807d7"
      },
      "cell_type": "markdown",
      "source": "I would like to see if there are any correlations between these velocities and accelerations and the velocities I measured in the vertical direction. I'll do this by sampling the horizontal velocity and acceleration data at a rate of once per second (every 30 frames) and averaging these values over the span of each rise and fall of the droplet, using the peaks and valleys found in the previous section."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "4f2ecd9299f4080fde2ff6e1a6d3e5e504412bd9"
      },
      "cell_type": "code",
      "source": "sample = np.arange(0, len(drop2), 30) # Sampling once per second\n\nsegments_down = [np.where((sample > peak2[i]) & (sample < val2[i])) for i in range(len(peak2))]\nsegments_up = [np.where((sample < peak2[i+1]) & (sample > val2[i])) for i in range(len(peak2)-1)]\n\nvels_up=[np.average([yvel.iloc[i] for i in segments_down[arr]]) for arr in range(len(peak2))]\nvels_down = [np.average([yvel.iloc[i] for i in segments_up[arr]]) for arr in range(len(peak2)-1)]\n\naccs_up=[np.average([yacc.iloc[i] for i in segments_down[arr]]) for arr in range(len(peak2))]\naccs_down = [np.average([yacc.iloc[i] for i in segments_up[arr]]) for arr in range(len(peak2)-1)]\n\nf, [ax1, ax2] = plt.subplots(nrows=2, ncols=1, figsize = (15, 15))\nax1.bar(np.arange(len(vels_down)), vels_down, alpha = 0.7)\nax1.bar(np.arange(len(vels_up)), vels_up, alpha = 0.7)\nax1.set(ylabel='Average horizontal velocity during rises and falls', \n       xticks=range(len(vels_up)))\nax1.legend(['Rises', 'Falls'], loc='lower right')\n\nax2.bar(np.arange(len(accs_down)), accs_down, alpha = 0.7)\nax2.bar(np.arange(len(accs_up)), accs_up, alpha = 0.7)\nax2.set(ylabel='Average horizontal acceleration during rises and falls', \n        xticks= range(len(vels_up)))\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a6bc1663587766322308a063728846e05a8e0c68"
      },
      "cell_type": "markdown",
      "source": "We start seeing larger than normal accelerations and velocities in the horizontal direction at about the 20th rise/fall. At about what frame does this change occur?"
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "a4e6a0e4348821c0493fccdf5fcfc72ae6551c06"
      },
      "cell_type": "code",
      "source": "print(peak2[20], val2[20])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59a45ea482c6634f758dac26207dbd7ecf81a313"
      },
      "cell_type": "markdown",
      "source": "This was a very interesting analysis, but tragically I did not find a way to incorporate these uncertainty findings into our final results. I really do believe that this form of analysis could yield very useful results and a eventually provide a way to gauge the certainty for a given velocity and charge measurement. If we had had more time in this lab I think this would have been a fruitful pursuit. At the very least, I believe this detour has succeeded in showing that the error introduced in the tracking of the particle is small, and supports our unorthodox method in this lab as being effective."
    },
    {
      "metadata": {
        "_uuid": "ba4f964c721006d02ccba1184bca9dd802619037"
      },
      "cell_type": "markdown",
      "source": "### Third droplet"
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "8e34e8dc76023e956ecd93e7b2a63e1852b1bc80"
      },
      "cell_type": "code",
      "source": "yvel = drop3.y.shift(-1)-drop3.y # Y velocity in one frame (dx)\n# Printing average magnitude of yvel (excluding final 100 frames)\nprint('Average horizontal velocity: %f' % np.average(np.sqrt(yvel[:-100]*yvel[:-100]))) \nf, ax = plt.subplots(figsize=(15, 10))\nyacc = yvel.shift(-1)-yvel # Y acceleration over one frame (dx^2)\n#Printing average magnitude of yacc (excluding final 100 frames)\nprint('Average horizontal acceleration: %f' % np.average(np.sqrt(yacc[:-100]*yacc[:-100])))\nax.plot(yvel, alpha = 0.5)\nax.plot(yacc, alpha=0.5)\nax.set(ylim = [-5, 5])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "757e99b7a242ec75316d7cfe79041235c4a62e43"
      },
      "cell_type": "code",
      "source": "sample = np.arange(0, len(drop3), 30)\n\nsegments_down = [np.where((sample > peak3[i]) & (sample < val3[i])) for i in range(len(peak3))]\nsegments_up = [np.where((sample < peak3[i+1]) & (sample > val3[i])) for i in range(len(peak3)-1)]\n\nvels_up=[np.average([yvel.iloc[i] for i in segments_down[arr]]) for arr in range(len(peak3))]\nvels_down = [np.average([yvel.iloc[i] for i in segments_up[arr]]) for arr in range(len(peak3)-1)]\n\naccs_up=[np.average([yacc.iloc[i] for i in segments_down[arr]]) for arr in range(len(peak3))]\naccs_down = [np.average([yacc.iloc[i] for i in segments_up[arr]]) for arr in range(len(peak3)-1)]\n\nf, [ax1, ax2] = plt.subplots(nrows=2, ncols=1, figsize = (15, 15))\nax1.bar(np.arange(len(vels_down)), vels_down, alpha = 0.7)\nax1.bar(np.arange(len(vels_up)), vels_up, alpha = 0.7)\nax1.set(ylabel='Average horizontal velocity during rises and falls', \n       xticks=range(len(vels_up)))\nax1.legend(['Rises', 'Falls'], loc='lower right')\n\nax2.bar(np.arange(len(accs_down)), accs_down, alpha = 0.7)\nax2.bar(np.arange(len(accs_up)), accs_up, alpha = 0.7)\nax2.set(ylabel='Average horizontal acceleration during rises and falls', \n        xticks= range(len(vels_up)))\n\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "2131a346bda3eae68c9ccef7307c57a01a2e4347"
      },
      "cell_type": "markdown",
      "source": "# Converting to Real World Units\nNow that we have all of our data in place, it's time to change units so that our values are actually physically meaningful.\n\n## First trial\nLet's convert the velocity values for our first trial to S.I. units and see their distribution again."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "0352a2fbdf15d915e51b43ff3266cba58f8818ea"
      },
      "cell_type": "code",
      "source": "vf1 = abs(np.array(vdown1) * 6.104*30*10**(-6)) #Using measured pixel-mm conversion rate\nvr1 = abs(np.array(vup1) * 6.104*30*10**(-6))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "b36caccae151009ce71716ceeae8e68a2f1c3555"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize = (15, 5))\nax.bar(np.arange(len(vf1)), vf1, alpha = 0.5)\nax.bar(np.arange(len(vr1)), vr1, alpha = 0.5, zorder=0)\nax.set(ylabel='Velocity (m/s)', xticks=np.arange(len(vr1)))\nax.legend(['$v_f$', '$v_r$'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a7da121db3d7e452da9267eac2f29c4375cffd4b"
      },
      "cell_type": "markdown",
      "source": "Now I will use the charge formula to convert these velocity values into charge values. I'll use the average fall velocity of all the trials to hopefully get a more accurate measurement of $v_f$, but I'll use the individual rising velocities for each charge calculation."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "9018d1e2b682227641ac74c3eb5aa6ebe53352d4"
      },
      "cell_type": "code",
      "source": "def charge1(vf, vr): \n    b = 8.2*10**(-3)\n    p = 101591.4 \n    eta = 1.8512 * 10 **(-5) \n    row = 886\n    g = 9.81\n    d = 0.0075\n    V = 500\n    \n    q = (4*np.pi/3)*((np.sqrt((b/(2*p))**2 + 9*eta*vf/(2*row*g))-b/(2*p))**3)*(row*g*d*(vf+vr))/(V*vf)\n    return q",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "25b3bb616a7599dec1b8a10811d48db111a93ad1"
      },
      "cell_type": "code",
      "source": "charges1 = np.array([charge1(np.average(vf1), v) for v in vr1])\nprint(np.average(charges1[charges1 < 4e-19])) # What's the average of my lowest set of values?\nf, ax = plt.subplots()\nax.bar(np.arange(len(charges1)), charges1)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d793761740ee20f2564bf594c8641f9f8b2b135"
      },
      "cell_type": "markdown",
      "source": "I want to find a constant such that all of my charge measurements are clustered around integer multiples of it. I'll do this by defining a residual function to be minimized, `res`, which returns the average squared difference between the charge measurements and their respective nearest integer multiple of $e$, for a given $e$ value. Just from scaling this by hand I know that the optimal value will lie between 1e-19 and 2e-19, so I'll create a range of values between these, spaced 0.001e-19 apart, and use whatever the minimum value is as my $e$ value."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "c590ae242ca00e44f80587f16b107a3f44131351"
      },
      "cell_type": "code",
      "source": "res = lambda elem: np.average(np.array([min([(charge-n*elem)**2 for n in np.arange(15)]) for charge in charges1]))\n\nxrange = np.arange(1e-19, 4e-19, 0.001e-19)\nvals = [res(x) for x in xrange]\nelem = min(xrange[np.where(vals==min(vals))])\nprint(elem)\nf, ax = plt.subplots()\nax.plot(xrange, [res(x) for x in xrange])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3be63b6e529186b3c1f44733aa9ed92bc1937cee"
      },
      "cell_type": "markdown",
      "source": "Not bad! Let's see what that does to our histogram."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "789da238bf9418da8de0847c9072b6e86b9e22b5"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(15, 5))\nax.hist(charges1/(elem), bins=50, align='mid')\nax.set(xticks=range(10))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e5556fa1c528cbffdfcbacb8094eaa2e601997c"
      },
      "cell_type": "markdown",
      "source": "Right away we see the expected behaviour. Our charge values are concentrated around integer multiples of some constant $e$. Let's see if we can reproduce this behaviour in our second trial."
    },
    {
      "metadata": {
        "_uuid": "1cfa5a2e92cc650b7c8809c25c54ef87b746f6d3"
      },
      "cell_type": "markdown",
      "source": "## Second Trial"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "51b607c6a376f503ad804053ec86530cf0bf39bb"
      },
      "cell_type": "code",
      "source": "vf2 = abs(np.array(vdown2) * 5.32*30*10**(-6))\nvr2 = abs(np.array(vup2) * 5.32*30*10**(-6))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "54ceada3bc1f0c96ee4d90b901c2513906f88720"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize = (15, 5))\nax.bar(np.arange(len(vf2)), vf2, alpha = 0.5)\nax.bar(np.arange(len(vr2)), vr2, alpha = 0.5, zorder=0)\nax.set(ylabel='Velocity (m/s)', xticks=np.arange(len(vr2)))\nax.legend(['$v_f$', '$v_r$'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "5837875ffe40a0956163e82fb4ca34c7da032cf6"
      },
      "cell_type": "code",
      "source": "def charge2(vf, vr): \n    b = 8.2*10**(-3)\n    p = 101591.4 \n    eta = 1.8512 * 10 **(-5) \n    row = 886\n    g = 9.81\n    d = 0.0075\n    V = 500\n    \n    q = (4*np.pi/3)*((np.sqrt((b/(2*p))**2 + 9*eta*vf/(2*row*g))-b/(2*p))**3)*(row*g*d*(vf+vr))/(V*vf)\n    return q",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "215f8a0f4e03e4a5f0888a5d384edcac8ac2b9cc"
      },
      "cell_type": "code",
      "source": "charges2 = np.array([charge2(np.average(vf2), v) for v in vr2])\nprint(np.average(charges2[charges2 < 2e-19])) # What's the average of my lowest set of values?\nf, ax = plt.subplots()\nax.bar(np.arange(len(charges2)), charges2)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6fa3dea55d9ecc2c1ba096eee75b9c993a4b2c2b"
      },
      "cell_type": "markdown",
      "source": "Now let's perform the same process as done in the first trial to find the optimal elementary charge value."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "b0ae3eeeb0537a0843bf9cadca38005650a5fc86"
      },
      "cell_type": "code",
      "source": "res = lambda elem: np.average(np.array([min([(charge-n*elem)**2 for n in np.arange(15)]) for charge in charges2]))\n\nxrange = np.arange(1e-19, 4e-19, 0.001e-19)\nvals = [res(x) for x in xrange]\nelem=min(xrange[np.where(vals==min(vals))])\nprint(elem)\nf, ax = plt.subplots()\nax.plot(xrange, [res(x) for x in xrange])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "846c2cd62084dd248b868c0c48ee064b99b61e02"
      },
      "cell_type": "markdown",
      "source": "Okay, now let's make a histogram using the calculated value for $e$, `elem`. "
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "4e5096640976591f6ecec6392b433b292c1dbb59"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(15, 5))\nax.hist(charges2/(elem), bins=100, align='mid')\nax.set(xticks=range(10))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d732ace57fd78d923784b1fe5b34eca6573789cb"
      },
      "cell_type": "markdown",
      "source": "Once again we see values clustering around integer multiples of some constant $e$. This is promising, although the calculated $e$ is somewhat lower than expected. Let's move on to trial 3."
    },
    {
      "metadata": {
        "_uuid": "337471f55c4eef5e613d0deced74e519d75b0d1d"
      },
      "cell_type": "markdown",
      "source": "## Third trial"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false,
        "_uuid": "480661cb761338dd92872db7fb3821e134a926ba"
      },
      "cell_type": "code",
      "source": "vf3 = abs(np.array(vdown3) * 1.9966*10**(-4))\nvr3 = abs(np.array(vup3) * 1.9966*10**(-4))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "70ec219e6c42306f69ec2825ad8ea5b97827e9a9"
      },
      "cell_type": "code",
      "source": "def charge3(vf, vr): \n    b = 8.2*10**(-3)\n    p = 101591\n    eta = 1.8463 * 10 **(-5) \n    row = 886\n    g = 9.81\n    d = 0.0075\n    V = 500\n    \n    q = (4*np.pi/3)*((np.sqrt((b/(2*p))**2 + 9*eta*vf/(2*row*g))-b/(2*p))**3)*(row*g*d*(vf+vr))/(V*vf)\n    return q",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "8bfe86e6f3ef2ec1ce2ee081b676af2772798469"
      },
      "cell_type": "code",
      "source": "charges3 = np.array([charge3(np.average(vf3), v) for v in vr3])\nprint(np.average(charges3[charges3 < 0.25e-18])) # What's the average of my lowest set of values?\nf, ax = plt.subplots()\nax.bar(np.arange(len(charges3)), charges3, alpha=0.7)\nax.set(ylim=[0, 1.6e-18])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "268e8de20c83a3340fbc3c193f5585e5f5de5454"
      },
      "cell_type": "code",
      "source": "res = lambda elem: np.average(np.array([min([(charge-n*elem)**2 for n in np.arange(15)]) for charge in charges3]))\n\nxrange = np.arange(1e-19, 3e-19, 0.001e-19)\nvals = [res(x) for x in xrange]\nelem=min(xrange[np.where(vals==min(vals))])\nprint(elem)\nf, ax = plt.subplots()\nax.plot(xrange, [res(x) for x in xrange])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "040673b748bc972cb69c8cf32c8d4f6c8115ebc0"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(15, 5))\nax.hist(charges3/(elem), bins=100, align='mid')\nax.set(xticks=range(10))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a96d0c7b53e8491287368a3c3ae143de6dd032d5"
      },
      "cell_type": "markdown",
      "source": "We see the same behaviour. Promising! Let's see what we get when we combine the datasets."
    },
    {
      "metadata": {
        "_uuid": "cc4e32d82f695a713cbcdebd56e6d81ceb8b702d"
      },
      "cell_type": "markdown",
      "source": "## Combined trials"
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "6260d1410f2c357c6fd987a5ed69c379d674fe35"
      },
      "cell_type": "code",
      "source": "charges = np.concatenate((charges1, charges2, charges3), axis=0)\n\nf, [ax1, ax2] = plt.subplots(nrows=2, ncols=1, figsize=(15, 10))\nax1.bar(np.arange(len(charges)), charges)\nax1.set(ylim=[0,1.5e-18])\nax2.hist(charges, bins=100)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "65e71a04131a8bc13abe7814094ea649cd4c04e1"
      },
      "cell_type": "code",
      "source": "res = lambda elem: np.average(np.array([min([(charge-n*elem)**2 for n in np.arange(30)]) for charge in charges]))\n\nxrange = np.arange(1.3e-19, 4e-19, 0.001e-19)\nvals = [res(x) for x in xrange]\nprint('Elementary charge: {0}'.format(min(xrange[np.where(vals==min(vals))])))\nprint('Standard deviation: {0}'.format(min(np.sqrt(np.array(vals)))))\nprint('Standard deviation as percentage of e: {0}'.format(min(np.sqrt(np.array(vals)))/min(xrange[np.where(vals==min(vals))])))\nf, ax = plt.subplots()\nax.plot(xrange, [res(x) for x in xrange])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a9bb9a9fad008c63c8b674744fdc06c9c30dcdd"
      },
      "cell_type": "markdown",
      "source": "Not a bad result! However that 18% standard deviation is cause for concern. I know my dataset contains some outliers which I did not remove earlier on, so let's see if I can remove some of these to improve this result. I'll look at the deviation for each charge in my dataset and see if there are any that are far enough away from the average to be considered outliers."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "30c959823dba857c2c6ec8af19b21924cab42a3e"
      },
      "cell_type": "code",
      "source": "dev = np.sqrt(np.array([min([(charge-n*1.606e-19)**2 for n in np.arange(30)]) for charge in charges]))\nplt.bar(range(len(charges)),dev)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72f6a605855100d9015daa11d7517a171bf8d955"
      },
      "cell_type": "markdown",
      "source": "It looks like there are about four charge values which deviate from the average much more than is typical. Let's see what happens if we remove the charges who deviate by more than 6e-20."
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "2b6ff8043657a3fe47c95d12e9be995072653e12"
      },
      "cell_type": "code",
      "source": "res = lambda elem: np.average(np.array([min([(charge-n*elem)**2 for n in np.arange(30)]) for charge in charges[dev < 6e-20]]))\n\nxrange = np.arange(1.3e-19, 4e-19, 0.001e-19)\nvals = [res(x) for x in xrange]\nprint('Elementary charge: {0}'.format(min(xrange[np.where(vals==min(vals))])))\nprint('Standard deviation: {0}'.format(min(np.sqrt(np.array(vals)))))\nprint('Standard deviation as percentage of e: {0}'.format(min(np.sqrt(np.array(vals)))/min(xrange[np.where(vals==min(vals))])))\nf, ax = plt.subplots()\nax.plot(xrange, [res(x) for x in xrange])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f59e73a9bb8590eb2afccb35038b19a1188a1c9d"
      },
      "cell_type": "markdown",
      "source": "Awesome. Let's plot our histogram while using this value for $e$. "
    },
    {
      "metadata": {
        "trusted": false,
        "collapsed": true,
        "_uuid": "48599b6d94c223d7013a762d2c9fada336d3df1f"
      },
      "cell_type": "code",
      "source": "f, ax = plt.subplots(figsize=(7, 4))\nnum=50\nax.hist(np.delete(charges1, [6])/1.606e-19, bins=num, alpha=0.5, range=(0, 10), zorder=2)\nax.hist(np.delete(charges2, [29-len(charges1),30-len(charges1),31-len(charges1)])/1.606e-19, bins=num, \n        range=(0, 10), alpha=0.5)\nax.hist(charges3/1.606e-19, bins=num, alpha=0.5, range=(0, 10), zorder=0)\n\nax.set(xticks=np.arange(10), xlim=[0,10], title='Histogram of droplet charge', \n       xlabel=r'Droplet charge / $e$', ylabel='Count number', yticks=range(0,13))\nax.legend(['Trial 1 (5 minutes)', 'Trial 2 (20 minutes)','Trial 3 (30 minutes)'])\nax.text(7, 8, r'$e=1.606 \\times 10^{-19}$ C')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fe00411e430d440a522b42927446a65d884331e6"
      },
      "cell_type": "markdown",
      "source": "Great! We see the expected behaviour. While imperfect, the histogram values spike around integer multiples of our measured $e$ value. "
    },
    {
      "metadata": {
        "_uuid": "46dfd84b9c81f1b29155da00c66c6ee6817a1b3a"
      },
      "cell_type": "markdown",
      "source": "# Conclusion\nIn this attempt to reproduce the Millikan experiment, the value of elementary charges was measured to be $1.6 \\times 10^{-19} \\pm 0.2 \\times 10^{-19}$ C, and a histogram was produced which supports the quantization of charge. The measured value was very close the generally accepted value of $e$, $1.602\\times 10^{-19}$ C. However, the large standard deviation of the charge measurements from the values predicted by the quantized model indicates a lack of precision which hampers these findings to a degree. This uncertainty could be remedied through a larger sample size, since in this experiment only the motion of three droplets was analyzed. Other possible ways of reducing this uncertainty include monitoring other parameters, such as air viscosity and voltage, with greater precision, and measuring the mass of the oil droplets through a more direct means than Stokes' Law."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}